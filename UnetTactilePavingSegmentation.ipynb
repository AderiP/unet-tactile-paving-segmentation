{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AderiP/unet-tactile-paving-segmentation/blob/main/UnetTactilePavingSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh-MgyQM2Qw9"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The U-Net model is a simple fully  convolutional neural network that is used for binary segmentation i.e foreground and background pixel-wise classification. Mainly, it consists of two parts. \n",
        "\n",
        "*   Contracting Path: we apply a series of conv layers and downsampling layers  (max-pooling) layers to reduce the spatial size \n",
        "*   Expanding Path: we apply a series of upsampling layers to reconstruct the spatial size of the input. \n",
        "\n",
        "The two parts are connected using a concatenation layers among different levels. This allows learning different features at different levels. At the end we have a simple conv 1x1 layer to reduce the number of channels to 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtX6zXly1szw"
      },
      "source": [
        "# トレーニングを始める前に\n",
        "このプログラムはデータセットをzipで圧縮して、Dropboxに置いて共有リンクからダウンロードしています。\n",
        "\n",
        "適宜、Googleドライブにデータセット置くなどして、データセットを取り込んでください。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ハイパーパラメータ"
      ],
      "metadata": {
        "id": "sW6zmfEg5nnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "SEED = 100\n",
        "TEST_GROUP = 0 # Number of the group used for test data (test_group=0~4)\n",
        "LEARNING_RATE = 0.0001\n",
        "MAX_EPOCHS = 50\n",
        "\n",
        "ADDITIONAL_LEARNING = False\n",
        "MODEL_PATH = \"/content/drive/My Drive/U-Net/\""
      ],
      "metadata": {
        "id": "z9QWGzjUwloK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSx_rcoo5Hj4"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a9Kq83Q3yb-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc135a1c-5357-404d-d0d0-f6cd6ac32788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LisvDshzevQ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bdeDl5HO0QsY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D\n",
        "from keras.layers import  Dropout, Activation, LeakyReLU\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "from random import shuffle\n",
        "import glob\n",
        "import csv\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check operating environment"
      ],
      "metadata": {
        "id": "i041-oZqqLz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!nvcc -V\n",
        "print()\n",
        "\n",
        "!python3 --version # -> Python 3.7.15\n",
        "print(\"TensorFlow version -> \",tf.__version__) # TensorFlow version ->  2.9.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFZmrhn8qK-S",
        "outputId": "15563be7-0f73-4647-e7f9-bb8229a5dde2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  7 19:44:39 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n",
            "\n",
            "Python 3.8.15\n",
            "TensorFlow version ->  2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgLQ71Qy4XxN"
      },
      "source": [
        "# Dataset copying from Dropbox and expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jgsiabPNJ4O-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://www.dropbox.com/s/a5wjrb866xn6b8h/img.zip\n",
        "!wget https://www.dropbox.com/s/6p7rwok1nx2zvo7/mask.zip\n",
        "!unzip \"mask\" & unzip \"img\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ax72zauu_I7O"
      },
      "outputs": [],
      "source": [
        "os.mkdir('img_index')\n",
        "os.mkdir('mask_index')\n",
        "\n",
        "index_img = glob.glob('img/**/*.jpg')\n",
        "for img in index_img:\n",
        "  shutil.copy(img,\"img_index/\" + os.path.basename(img))\n",
        "\n",
        "index_mask = glob.glob('mask/**/*.png')\n",
        "for mask in index_mask:\n",
        "  shutil.copy(mask,\"mask_index/\" + os.path.basename(mask))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl-AZhZPStVE"
      },
      "source": [
        "# Write to hyperparameters text file & Set seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ANiuz_yKSyx-"
      },
      "outputs": [],
      "source": [
        "time = datetime.datetime.now()\n",
        "now_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "if ADDITIONAL_LEARNING:\n",
        "  RESULT_DIR = f\"/content/drive/My Drive/U-Net/{now_time} t={TEST_GROUP:d}:add\"\n",
        "else:\n",
        "  RESULT_DIR = f\"/content/drive/My Drive/U-Net/{now_time} t={TEST_GROUP:d}\"\n",
        "\n",
        "if not os.path.exists(RESULT_DIR):  # ディレクトリが存在しない場合、作成する。\n",
        "    os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "if ADDITIONAL_LEARNING:\n",
        "  textfile_name = os.path.join(RESULT_DIR,  f\"{now_time} t={TEST_GROUP:d}:add parameter.txt\")\n",
        "else:\n",
        "  textfile_name = os.path.join(RESULT_DIR,  f\"{now_time} t={TEST_GROUP:d} parameter.txt\")\n",
        "\n",
        "with open(textfile_name, \"w\") as f:\n",
        "  f.write(f\"BATCH_SIZE: {str(BATCH_SIZE)}\\n\")\n",
        "  f.write(f\"SEED: {str(SEED)}\\n\")\n",
        "  f.write(f\"TEST_GROUP: {str(TEST_GROUP)}\\n\")\n",
        "  f.write(f\"LEARNING_RATE: {str(LEARNING_RATE)}\\n\")\n",
        "  f.write(f\"MAX_EPOCHS: {str(MAX_EPOCHS)}\\n\")\n",
        "  f.write(f\"ADDITIONAL_LEARNING: {str(ADDITIONAL_LEARNING)}\\n\")\n",
        "\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tavQ2-MJ0x9E"
      },
      "source": [
        "# Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P8tq55g677wp"
      },
      "outputs": [],
      "source": [
        "def image_generator(files, batch_size = 32, sz = (256, 256)):\n",
        "  \n",
        "  while True: \n",
        "    \n",
        "    #extract a random batch \n",
        "    batch = np.random.choice(files, size = batch_size)    \n",
        "    \n",
        "    #variables for collecting batches of inputs and outputs \n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    \n",
        "    \n",
        "    for f in batch:\n",
        "\n",
        "        #get the masks. Note that masks are png files \n",
        "        mask = Image.open(f'mask_index/{f[:-4]}.png')\n",
        "        mask = np.array(mask.resize(sz))\n",
        "\n",
        "        #preprocess the mask \n",
        "        # mask[mask >= 2] = 0 \n",
        "        mask[mask != 0 ] = 1\n",
        "        \n",
        "        batch_y.append(mask)\n",
        "\n",
        "        #preprocess the raw images \n",
        "        raw = Image.open(f'img_index/{f}')\n",
        "        raw = raw.resize(sz)\n",
        "        raw = np.array(raw)\n",
        "\n",
        "        #check the number of channels because some of the images are RGBA or GRAY\n",
        "        if len(raw.shape) == 2:\n",
        "          raw = np.stack((raw,)*3, axis=-1)\n",
        "\n",
        "        else:\n",
        "          raw = raw[:,:,0:3]\n",
        "\n",
        "        batch_x.append(raw)\n",
        "\n",
        "    #preprocess a batch of images and masks \n",
        "    batch_x = np.array(batch_x)/255.\n",
        "    batch_y = np.array(batch_y)\n",
        "    batch_y = np.expand_dims(batch_y,3)\n",
        "\n",
        "    yield (batch_x, batch_y)      \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj1HFRykCl8v"
      },
      "source": [
        "# Set up training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Ng9egOQy9j",
        "outputId": "02c62294-2187-4552-9922-b21a0c337fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order of sequence directory\n",
            "['17', '09', '15', '20', '01', '07', '04', '13', '19', '02', '16', '03', '12', '06', '10', '14', '18', '08', '05', '11']\n",
            "\n",
            "test sequence: ['17', '09', '15', '20']\n",
            "traint sequence: ['01', '07', '04', '13']\n",
            "traint sequence: ['19', '02', '16', '03']\n",
            "traint sequence: ['12', '06', '10', '14']\n",
            "traint sequence: ['18', '08', '05', '11']\n",
            "\n",
            "first img\n",
            "train sequence: VID_20210906_153203_0971.jpg\n",
            "test sequence: VID_20210809_121009_0616.jpg\n"
          ]
        }
      ],
      "source": [
        "seq_dirs = os.listdir('img')\n",
        "shuffle(seq_dirs)\n",
        "\n",
        "print(\"Order of sequence directory\")\n",
        "print(seq_dirs)\n",
        "print()\n",
        "\n",
        "# Group of 5 divided datasets\n",
        "img_groups = []\n",
        "img_group = []\n",
        "seq_group = []\n",
        "\n",
        "for i, seq_dir in enumerate(seq_dirs, 1):\n",
        "  img_group.extend(os.listdir('img/'+seq_dir))\n",
        "  seq_group.append(seq_dir)\n",
        "  if i % 4 == 0:\n",
        "    img_groups.append([img_group,seq_group])\n",
        "    img_group = []\n",
        "    seq_group = []\n",
        "\n",
        "train_files = []\n",
        "test_files = []\n",
        "\n",
        "for i in range(5):\n",
        "  if i == TEST_GROUP:\n",
        "    print(\"test sequence: \", end=\"\")\n",
        "    print(img_groups[i][1])\n",
        "\n",
        "    test_files = img_groups[i][0]\n",
        "  else:\n",
        "    print(\"traint sequence: \", end=\"\")\n",
        "    print(img_groups[i][1])\n",
        "\n",
        "    train_files.extend(img_groups[i][0])\n",
        "\n",
        "shuffle(train_files)\n",
        "shuffle(test_files) \n",
        "\n",
        "print(\"\\nfirst img\")\n",
        "print(\"train sequence: \", end=\"\")\n",
        "print(train_files[0])\n",
        "print(\"test sequence: \", end=\"\")\n",
        "print(test_files[0])\n",
        "\n",
        "train_generator = image_generator(train_files, batch_size = BATCH_SIZE)\n",
        "test_generator  = image_generator(test_files, batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raXLOfBk1pLV"
      },
      "source": [
        "# IoU metric\n",
        "\n",
        "The intersection over union (IoU) metric is a simple metric used to evaluate the performance of a segmentation algorithm. Given two masks $y_{true}, y_{pred}$ we evaluate \n",
        "\n",
        "$$IoU = \\frac{y_{true} \\cap y_{pred}}{y_{true} \\cup y_{pred}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xJLriYXX1oZU"
      },
      "outputs": [],
      "source": [
        "def mean_iou(y_true, y_pred):\n",
        "    yt0 = y_true[:,:,:,0]\n",
        "    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
        "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
        "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
        "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu-dv9PbzEmz"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7RY3B8yK828Z"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "\n",
        "    smooth = 1. # ゼロ除算回避のための定数\n",
        "    y_true_flat = tf.reshape(y_true, [-1]) # 1次元に変換\n",
        "    y_pred_flat = tf.reshape(y_pred, [-1]) # 同様\n",
        "\n",
        "    tp = tf.reduce_sum(y_true_flat * y_pred_flat) # True Positive\n",
        "    nominator = 2 * tp + smooth # 分子\n",
        "    denominator = tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat) + smooth # 分母\n",
        "    score = nominator / denominator\n",
        "    return 1. - score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160g6Ex41r-2"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hONrrUbW9CM_"
      },
      "outputs": [],
      "source": [
        "def unet(sz = (256, 256, 3)):\n",
        "  x = Input(sz)\n",
        "  inputs = x\n",
        "  \n",
        "  #down sampling \n",
        "  f = 8\n",
        "  layers = []\n",
        "  \n",
        "  for i in range(0, 6):\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "    layers.append(x)\n",
        "    x = MaxPooling2D() (x)\n",
        "    f = f*2\n",
        "  ff2 = 64 \n",
        "  \n",
        "  #bottleneck \n",
        "  j = len(layers) - 1\n",
        "  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "  x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n",
        "  x = Concatenate(axis=3)([x, layers[j]])\n",
        "  j = j -1 \n",
        "  \n",
        "  #upsampling \n",
        "  for i in range(0, 5):\n",
        "    ff2 = ff2//2\n",
        "    f = f // 2 \n",
        "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "    x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n",
        "    x = Concatenate(axis=3)([x, layers[j]])\n",
        "    j = j -1 \n",
        "    \n",
        "  \n",
        "  #classification \n",
        "  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "  outputs = Conv2D(1, 1, activation='sigmoid') (x)\n",
        "\n",
        "  #optimaizer\n",
        "  # rmsprop = RMSprop(learning_rate=LEARNING_RATE, rho=0.9, epsilon=None, decay=0.0)\n",
        "  adam = Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "  #model creation \n",
        "  model = Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer = adam, loss = dice_loss, metrics = [mean_iou])\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "67Fyeczk_zzh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "919863d3-8020-4836-9503-e5b7bcde7583"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7e0871314ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mADDITIONAL_LEARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (file read failed: time = Wed Dec  7 19:46:25 2022\n, filename = '/content/drive/My Drive/U-Net/', file descriptor = 65, errno = 21, error message = 'Is a directory', buf = 0x7ffe467b6e88, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)"
          ]
        }
      ],
      "source": [
        "model = unet()\n",
        "\n",
        "if ADDITIONAL_LEARNING:\n",
        "  model.load_weights(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ModelCheckpoint"
      ],
      "metadata": {
        "id": "AOd7oLTY6j6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_DIR = os.path.join(RESULT_DIR, \"checkpoint\")\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "if ADDITIONAL_LEARNING:\n",
        "  checkpoint_filepath = os.path.join(CHECKPOINT_DIR, f\"{now_time} t={TEST_GROUP:d}:add \" + \"{epoch:03d}.h5\")\n",
        "else:\n",
        "  checkpoint_filepath = os.path.join(CHECKPOINT_DIR, f\"{now_time} t={TEST_GROUP:d} \" + \"{epoch:03d}.h5\")\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                             save_best_only=True,\n",
        "                             save_weights_only=False,\n",
        "                             save_freq=10)"
      ],
      "metadata": {
        "id": "dfEH8NnF6oXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7QY8rgO1zUU"
      },
      "source": [
        "# Callbacks\n",
        "\n",
        "Simple functions to save the model at each epoch and show some predictions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfqXmNuc9lWZ"
      },
      "outputs": [],
      "source": [
        "LOG_DIR = f\"/content/drive/My Drive/U-Net/logs\"\n",
        "\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "if ADDITIONAL_LEARNING:\n",
        "  log_dir_path = os.path.join(LOG_DIR, f\"{now_time} t={TEST_GROUP:d}:add\")\n",
        "else:\n",
        "  log_dir_path = os.path.join(LOG_DIR, f\"{now_time} t={TEST_GROUP:d}\")\n",
        "\n",
        "def build_callbacks():\n",
        "        checkpointer = checkpoint\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_path)\n",
        "        callbacks = [checkpointer, PlotLearning(), tensorboard_callback]\n",
        "        return callbacks\n",
        "\n",
        "# inheritance for training process plot \n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        self.logs = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('mean_iou'))\n",
        "        self.val_acc.append(logs.get('val_mean_iou'))\n",
        "        self.i += 1\n",
        "        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'mean_iou=',logs.get('mean_iou'),'val_mean_iou=',logs.get('val_mean_iou'))\n",
        "        \n",
        "        #choose a random test image and preprocess\n",
        "        path = np.random.choice(test_files)\n",
        "        raw = Image.open(f'img_index/{path}')\n",
        "        raw = np.array(raw.resize((256, 256)))/255.\n",
        "        raw = raw[:,:,0:3]\n",
        "        \n",
        "        #predict the mask \n",
        "        pred = model.predict(np.expand_dims(raw, 0))\n",
        "        \n",
        "        #mask post-processing \n",
        "        msk  = pred.squeeze()\n",
        "        msk = np.stack((msk,)*3, axis=-1)\n",
        "        msk[msk >= 0.5] = 1\n",
        "        msk[msk < 0.5] = 0\n",
        "        \n",
        "        #show the mask and the segmented image \n",
        "        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(combined)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU6SPuVY8Mdc"
      },
      "source": [
        "# Training Executor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MXGinNg9Wjj"
      },
      "outputs": [],
      "source": [
        "def training():\n",
        "  train_steps = len(train_files) //BATCH_SIZE\n",
        "  test_steps = len(test_files) //BATCH_SIZE\n",
        "  history = model.fit(train_generator, \n",
        "                      epochs = MAX_EPOCHS,\n",
        "                      steps_per_epoch = train_steps,\n",
        "                      validation_data = test_generator, \n",
        "                      validation_steps = test_steps, \n",
        "                      callbacks = build_callbacks())\n",
        "  hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "  if ADDITIONAL_LEARNING:\n",
        "    history_csv_path = os.path.join(RESULT_DIR, f\"{now_time} t={TEST_GROUP:d}:add history.csv\")\n",
        "  else:\n",
        "    history_csv_path = os.path.join(RESULT_DIR, f\"{now_time} t={TEST_GROUP:d} history.csv\")\n",
        "\n",
        "\n",
        "  hist_df.to_csv(os.path.join(RESULT_DIR, f\"{now_time} t={TEST_GROUP:d} history.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBXvj5yZHEg9"
      },
      "source": [
        "# Test Executor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLlWNYg8HHPP"
      },
      "outputs": [],
      "source": [
        "def testing():\n",
        "  print(\"\\ntest data mean iou\")\n",
        "\n",
        "  if ADDITIONAL_LEARNING:\n",
        "    csv_path = os.path.join(RESULT_DIR, f\"{now_time} t={TEST_GROUP:d}:add result.csv\")\n",
        "  else:\n",
        "    csv_path = os.path.join(RESULT_DIR, f\"{now_time} t={TEST_GROUP:d} result.csv\")\n",
        "\n",
        "  with open(csv_path, 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([\"img_name\",\"iou\"])\n",
        "\n",
        "      iou_sum = 0.0\n",
        "\n",
        "      for i, test_file in enumerate(test_files,1):\n",
        "        mask = Image.open(f'mask_index/{test_file[:-4]}.png')\n",
        "        mask = np.array(mask.resize((256, 256)))\n",
        "\n",
        "        #preprocess the mask \n",
        "        mask[mask != 0 ] = 1\n",
        "\n",
        "        raw = Image.open(f'img_index/{test_file}')\n",
        "        raw = np.array(raw.resize((256, 256)))/255.\n",
        "        raw = raw[:,:,0:3]\n",
        "\n",
        "        #predict the mask \n",
        "        pred = model.predict_on_batch(np.expand_dims(raw, 0))\n",
        "\n",
        "        yt0 = K.cast(mask, 'float32')\n",
        "        yp0 = K.cast(pred[:,:,:,0] > 0.5, 'float32')\n",
        "        inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
        "        union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
        "        iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
        "\n",
        "        writer.writerow([test_file, iou.numpy()])\n",
        "\n",
        "        iou_sum = iou_sum + iou.numpy()\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "          print(\"[{:d}/{:d}] {:f}\".format(i, len(test_files), iou_sum/i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X3KWuQQEaLZ"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDpejX-9EZR8"
      },
      "outputs": [],
      "source": [
        "training()\n",
        "\n",
        "if ADDITIONAL_LEARNING:\n",
        "  model_name = os.path.join(RESULT_DIR, f\"{now_time} t={TEST_GROUP:d}:add .h5\")\n",
        "else:\n",
        "  model_name = os.path.join(RESULT_DIR, f\"{now_time} t={TEST_GROUP:d}.h5\")\n",
        "\n",
        "model.save(model_name)\n",
        "\n",
        "testing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPgk674T63XM"
      },
      "source": [
        "# TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMf1FQ5J64hT"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/My Drive/U-Net/logs\" \".\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QgLQ71Qy4XxN"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}